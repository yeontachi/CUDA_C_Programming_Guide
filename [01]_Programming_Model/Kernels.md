# Kernels
**CUDA C++**는 일반적인 C++ 언어를 확장하여, 개발자가 **커널(kernel)**이라 불리는 함수를 정의할 수 있도록 한다.

이 커널 함수는 한 번 호출될 때 **수많은 스레드(thread)**에 의해 동시에 실행된다.

즉, CPU에서 작성한 일반적인 함수가 한 번만 실행되는 것과 달리, 
커널은 GPU에서 **N개의 스레드가 병렬로 한 번씩 수행**한다는 점이 가장 큰 차이이다.

## 커널 함수의 정의
커널은 `__global__` 키워드를 사용하여 정의한다.
이 키워드는 "이 함수는 GPU에서 실행되지만, CPU에서 호출할 수 있다."는 의미를 가진다.

아래 예제는 두 벡터 A와 B를 더하여 결과를 C에 저장하는 간단한 커널 코드이다.
```cpp
// GPU에서 실행될 함수 (커널)
__global__ void VecAdd(float* A, float* B, float* C)
{
    int i = threadIdx.x;       // 현재 스레드의 고유 인덱스
    C[i] = A[i] + B[i];        // 각 스레드는 한 원소씩 더함
}

int main(void)
{
    ...
    VecAdd<<<1, N>>>(A, B, C)   // 커널 호출: 1개의 블록, N개의 스레드
    ...
}
```
여기서 `<<<1, N>>>`은 **실행 구성(Execution Configuration)**을 나타낸다.
첫 번째 인자 `1`은 **블록(block)**의 개수,
두 번째 인자 `N`은 **각 블록 안의 스레드 수**를 의미한다.

따라서 이 코드는 "하나의 블록 안에서 N개의 스레드가 병렬로 VecAdd를 수행하라"는 의미이다.

## GPU에서의 실제 실행 구조
커널이 호출되면, CUDA 런타임은 GPU에 명령을 전달하고 GPU는 곧바로 수천 개의 스레드를 생성한다. 
이 스레드들은 **Streaming Multiprocessor(SM)**라 불리는 GPU의 연산 단위 안에서 동작한다.
각 SM은 다수의 **CUDA 코어**와 **자체적인 레지스터**, **공유 메모리(Shared Memory)**를 가지고 있으며,
GPU 내부에서 실제로 연산을 수행하는 핵심 단위가 된다.

스레드들은 SM 내부에서 **워프(warp)** 단위로 묶인다. 워프는 32개의 스레드로 구성되며, 하나의 명령어를 동시에 실행한다. 이를 **SIMT(Single Instruction, Multiple Threads)** 구조라고 한다.

즉, 동일한 명령어를 동시에 여러 데이터에 적용하여 높은 연산량을 처리할 수 있는 것이다.

이 구조 덕분에 GPU는 단일 CPU보다 훨씬 더 높은 병렬 처리량(throughput)을 제공한다.

예를 들어, 벡터 덧셈에서는 각 스레드가 서로 다른 원소를 담당하므로, 
모든 원소가 동시에 더해지며 CPU의 반복문보다 훨씬 빠르게 결과를 얻을 수 있다.

## 스레드 식별과 데이터 매핑
GPU에서 실행되는 각 스레드는 자신만의 고유한 인덱스를 가진다.
이 인덱스는 내장 변수인 `threadIdx`를 통해 접근할 수 있다.

`threadIdx.x`는 블록 내에서의 스레드 위치를 나타내며,
예를 들어 `threadIdx.x==0`인 스레드는 첫 번째 원소,
`threadIdx.x==1`인 스레드는 두 번째 원소를 처리한다.

이처럼 각 스레드는 자신에게 해당하는 인덱스의 데이터를 가져와 연산을 수행하고,
연산이 끝나면 GPU 메모리(global memory)에 결과를 저장한다.

## CPU와 GPU의 협력 관계
CUDA 프로그램의 실행은 일반적으로 다음 단계를 따른다.

**1.** **CPU(Host)**에서 데이터를 준비하고 GPU 메모리를 할당한다.

**2.** **cudaMemcpy()** 함수를 이용해 데이터를 CPU에서 GPU로 복사한다.

**3.** 커널을 호출하여 GPU에서 병렬 연산을 수행한다.

**4.** 연산이 완료되면 **결과 데이터를 GPU에서 CPU로 복사한다.

**5.** GPU 메모리를 해제한다.

이 과정에서 CPU는 "작업 지시자(Controller)"의 역할을 하며,
GPU는 "실제 계산을 담당하는 대규모 병렬 처리 장치"로 동작한다.
즉, CPU는 단 한 번 명령을 내리지만, GPU는 그 명령을 수천 개의 스레드가 동시에 수행하는 것이다.

이러한 병렬 구조는 GPU의 본질적인 장점을 극대화한다.
CPU가 하나의 명령을 순차적으로 처리한다면, GPU는 같은 명령을 수천 번 동시에 실행한다.

이를 통해 벡터, 행렬, 영상, 그리고 딥러닝 연산과 같은 데이터 병렬성이 높은 작업에서 탁월한 성능을 발휘할 수 있다.

GPU의 설계 철학은 "단일 작업의 지연(latency)을 줄이는 것"이 아니라, **동시에 처리할 수 있는 작업의 수를 극대화하는 것"**이다.
즉, Throughput-Oriented Architecture 이다.

## Summary

| 구분               | 설명                           |
| ---------------- | ---------------------------- |
| **커널(kernel)**   | GPU에서 병렬로 실행되는 함수            |
| **`__global__`** | CPU에서 호출 가능, GPU에서 실행됨       |
| **`<<<...>>>`**  | 커널의 실행 구성을 지정 (그리드와 블록 크기)   |
| **`threadIdx`**  | 각 스레드의 고유 인덱스                |
| **실행 방식**        | GPU는 각 스레드를 SM에 배치해 병렬 실행    |
| **장점**           | 수천 개의 연산을 동시에 수행하여 높은 처리량 확보 |

>CUDA 커널은 GPU 상에서 수많은 스레드가 동시에 수행하는 함수이며,
각 스레드는 자신에게 할당된 데이터 조각을 독립적으로 처리한다.
이러한 구조를 통해 GPU는 방대한 양의 연산을 병렬로 수행하며,
CPU보다 훨씬 높은 연산 처리 효율을 달성한다.